from dataclasses import asdict, dataclass
from enum import StrEnum, auto
from pathlib import Path
from typing import Literal, Optional, assert_never

from eliot import start_action

from ok.config import ConfigModel
from ok.constants import PLAN_FILE
from ok.env import Env
from ok.git_utils import has_uncommitted_changes
from ok.llm import check_verdict
from ok.llms.base import LLMBase
from ok.log import LLMOutputType
from ok.task_planning import planning_phase
from ok.util.eliot import log_call
from ok.utils import format_tool_code_output


"""
Manages the iterative step phase of a task, including code generation, execution, and evaluation.

Overview of the state machine:

- we begin a "step"
- we make an "attempt" to implement the step
- we judge the attempt
    - if the attempt is successful, we will be judging the step next
    - if the attempt is partial, we will be making another attempt
    - if the attempt is a failure, we will be making another attempt unless some limits are reached
- once we *either* exhausted the attempts or the last attempt was successful, we judge whether the step is complete
    - if the step is complete, we commit the changes
    - if the step is not complete, we will be making another attempt
- once the step is complete, we will be judging whether the task is complete
    - if the task is complete, we will be returning to the main loop
    - if the task is not complete, we will be starting a new step
"""


class StepVerdict(StrEnum):
    """Enum for possible verdicts from the step judge."""

    SUCCESS = auto()
    """Work done here is a good step forward."""
    PARTIAL = auto()
    """
    Keep work done in this step so far, but it needs more iteration.
    """
    AUTO_CHECK_FAILED = auto()
    """
    Automated check (e.g. lint, tests) failed, distinct from PARTIAL.
    """
    FAILURE = auto()
    """Work done in this step is not useful and should be discarded."""


class TaskVerdict(StrEnum):
    """Enum for possible verdicts from the task completion judge."""

    COMPLETE = auto()
    """Task is fully completed."""
    CONTINUE = auto()
    """More work is needed."""


@dataclass(frozen=True, slots=True)
class AttemptResult:
    """Result of a single attempt in a step."""

    verdict: Optional[StepVerdict]
    feedback: Optional[str]


@dataclass(frozen=True, slots=True)
class StepResult:
    """Result we get after completing a *step*."""

    history: list[AttemptResult]
    verdict: TaskVerdict | Literal["failed"]
    feedback: Optional[str] = None


# Base state classes


@dataclass(frozen=True, slots=True)
class TaskState:
    """Base class for all states."""

    plan: str
    """Plan for the task, generated by the LLM."""

    steps_log: list[StepResult]
    """Previous steps' results."""


@dataclass(frozen=True, slots=True)
class StepState(TaskState):
    """Base class for all states happening while working on a step."""

    attempts_log: list[AttemptResult]
    """Previous attempts' results."""


@dataclass(frozen=True, slots=True)
class AttemptState(StepState):
    pass


# State machine states
@dataclass(frozen=True, slots=True)
class RefiningPlan(TaskState):
    """
    State for refining the plan after completion judge feedback.
    """

    feedback: str


@dataclass(frozen=True, slots=True)
class StartingTask:
    """
    Represents the initial state where the agent is ready to start working on a task.
    """


@dataclass(frozen=True, slots=True)
class StartingStep(TaskState):
    """
    Represents the state where the agent is ready to start working on a step.
    """


@dataclass(frozen=True, slots=True)
class StartingAttempt(StepState):
    """Represents the state where the agent is "inside" a step and is ready to make an attempt."""


@dataclass(frozen=True, slots=True)
class PostAttemptHooks(AttemptState):
    """
    The agent is going to run post-attempt hooks like `post-implementation-hook-command`
    and `post-implementation-check-command`.
    """

    attempt_summary: Optional[str]


@dataclass(frozen=True, slots=True)
class JudgingAttempt(AttemptState):
    """
    Represents the state where the agent reviews the work done and whether more attempts are needed.

    From here we can go to:
    - `MakingAttempt` to make another attempt if the step is not done yet
    - `JudgingStep` to see if the task is done or we need more steps
    """

    attempt_summary: Optional[str]
    """
    Natural‑language summary of the work done in the current attempt.

    `None` if the attempt failed to produce a summary, which may happen even if code changes were made.
    """


@dataclass(frozen=True, slots=True)
class JudgingStep(StepState):
    """
    After a successful step, commit the changes and ask whether the overall task is finished.

    From here we can go to:
    - `StartingStep` to start a new step if the task is not done yet
    - `FinalizingTask` to finalize the task
    """


@dataclass(frozen=True, slots=True)
class FinalizingTask(TaskState):
    """
    Maybe we need to do a few more things before saying we are "done".
    """

    verdict: TaskVerdict | Literal["failed"]
    status: Optional[str]


@dataclass(frozen=True, slots=True)
class Done:
    """
    Ok, now we're done.
    """

    verdict: TaskVerdict | Literal["failed", "interrupted"]
    status: Optional[str]


type State = (
    StartingTask
    | StartingStep
    | StartingAttempt
    | PostAttemptHooks
    | JudgingAttempt
    | JudgingStep
    | RefiningPlan
    | FinalizingTask
    | Done
)

# Events


@dataclass(frozen=True, slots=True)
class Tick:
    """Keep going."""


type Event = Tick


@dataclass(frozen=True, slots=True)
class Settings:
    env: Env
    task: str
    base_commit: str
    cwd: Path
    llm: LLMBase
    config: ConfigModel


async def transition(
    state: State,
    event: Event,
    settings: Settings,
) -> State:
    """
    single‑step transition for the task‑execution state‑machine

    all long‑running side‑effects (llm calls, git commands, etc.) are executed
    inside the relevant branches, so the caller only needs to keep feeding
    events until a terminal state (`Complete` or `Failed`) is reached
    """

    with start_action(
        action_type=f"transition({state.__class__.__name__}, {event.__class__.__name__})",
        **({"state": asdict(state)} if asdict(state) else {}),
        **({"event": asdict(event)} if asdict(event) else {}),
    ) as action:
        match state, event:
            case StartingTask(), Tick():
                # TODO: this can actually be a class method?
                result = await _handle_StartingTask(settings, state)

            case StartingStep(), Tick():
                result = _handle_StartingStep(settings, state)

            case StartingAttempt(), Tick():
                result = await _handle_StartingAttempt(settings, state)

            case PostAttemptHooks(), Tick():
                result = await _handle_PostAttemptHooks(settings, state)

            case JudgingAttempt(), Tick():
                result = await _handle_JudgingAttempt(settings, state)

            case JudgingStep(), Tick():
                result = await _handle_JudgingStep(settings, state)

            case RefiningPlan(), Tick():
                result = await _handle_RefiningPlan(settings, state)

            case FinalizingTask(), Tick():
                result = await _handle_FinalizingTask(settings, state)

            case Done(), Tick():
                settings.env.log_debug("Done state reached, no further transitions.")
                result = state

            case _, _:
                assert_never(state)
                assert_never(event)

        action.addSuccessFields(_=result.__class__.__name__, **asdict(result))

        return result


async def _handle_StartingTask(settings: Settings, state: StartingTask) -> StartingStep | Done:
    """
    Generate a plan for the task.
    """

    # TODO: move into the same state machine?

    plan = await planning_phase(settings.env, llm=settings.llm, task=settings.task, cwd=settings.cwd)
    if not plan:
        settings.env.log("Failed to generate a plan for the step", message_type=LLMOutputType.ERROR)
        return Done(
            verdict="failed",
            status="Failed to generate a plan for the step",
        )
    else:
        settings.env.log("Plan generated successfully", message_type=LLMOutputType.STATUS)
        return StartingStep(
            plan=plan,
            steps_log=[],
        )


def _handle_StartingStep(settings: Settings, state: StartingStep) -> StartingAttempt:
    """
    Start a new step with the given plan.
    """

    return StartingAttempt(
        plan=state.plan,
        steps_log=state.steps_log,
        attempts_log=[],
    )


async def _handle_StartingAttempt(settings: Settings, state: StartingAttempt) -> PostAttemptHooks:
    """
    Generate the implementation prompt for a single step, invoke the LLM,
    and return its summary of work done.

    This may be rerun multiple times until it succeeds.
    Each run is called an "attempt".
    """

    from ok.log import format_as_markdown_blockquote

    settings.env.log_debug(
        "StartingAttempt, previous attempt was:",
        previous_attempt=state.attempts_log[-1] if state.attempts_log else None,
    )

    prev_attempt_feedback = (
        f"And this feedback from the last iteration:\n\n{format_as_markdown_blockquote(state.attempts_log[-1].feedback or '')}\n\n"
        if state.attempts_log
        else ""
    )

    # TODO: get rid of settings, they can all be in TaskState
    impl_prompt = (
        f"Execution phase. You are implementing this task: {repr(settings.task)}.\n"
        f"This is your attempt #{len(state.attempts_log) + 1}.\n"
        "\n"
        "Based on this plan:\n"
        "\n"
        f"{state.plan}\n"
        "\n"
        f"{prev_attempt_feedback}"
        f"Decide on, and implement the next step for task {repr(settings.task)}.\n"
        "Create files, run commands, and/or write code as needed.\n"
        "After you are done, output a summary of your activities as a single message using this template:\n\n"
        "    I am the task implementor.\n\n"
        "    List of issues reported about the last iteration, and how I addressed them: [[list, or 'None']]\n\n"
        "    Brief summary of code changes made in this step, and why: [[your summary, or 'None']]\n\n"
        "    Other work done in this step that contributed to the task: [[your summary, or 'None']]\n\n"
        "    Possible next step to take, according to the plan: [[your proposal, or 'I think the task is done']]\n\n"
        "    End of summary.\n\n"
    )

    if settings.config.implement.extra_prompt:
        impl_prompt += f"\n\n{settings.config.implement.extra_prompt}"

    settings.env.update_status("Implementing a step")

    attempt_summary = await settings.llm.run(
        settings.env, impl_prompt, yolo=True, cwd=settings.cwd, response_type=LLMOutputType.LLM_RESPONSE
    )

    return PostAttemptHooks(
        plan=state.plan,
        steps_log=state.steps_log,
        attempts_log=state.attempts_log,
        attempt_summary=attempt_summary,
    )


async def _handle_PostAttemptHooks(settings: Settings, state: PostAttemptHooks) -> JudgingAttempt | StartingAttempt:
    # This one always runs and is supposed to have things like formatters, etc.
    if settings.config.post_implementation_hook_command:
        await settings.env.run(
            settings.config.post_implementation_hook_command,
            "Running post-step hook",
            directory=settings.cwd,
            shell=True,
            run_timeout_seconds=settings.config.run_timeout_seconds,
        )

    # This one *checks* code after the attempt. If it fails, we will make another attempt.
    if settings.config.post_implementation_check_command:
        check_result = await settings.env.run(
            settings.config.post_implementation_check_command,
            "Running post-implementation check",
            directory=settings.cwd,
            shell=True,
            run_timeout_seconds=settings.config.run_timeout_seconds,
        )
        if check_result.exit_code != 0:
            feedback = (
                f"Post-implementation check command failed with exit code {check_result.exit_code}.\n"
                + format_tool_code_output(check_result)
                + "\n\n"
                "Note that this is an automated check, not a judge reviewing your work.\n"
                "You will *not* be able to proceed to the next step until this check passes.\n"
                "Your next step is either to fix the issues reported by the check, or to undo your changes and try a different approach.\n"
                "If the failure is expected, e.g. an expected test failure, "
                "you still have to find some other way to report the failure without triggering the check.\n"
            )
            attempt_result = AttemptResult(
                verdict=StepVerdict.AUTO_CHECK_FAILED,
                feedback=feedback,
            )
            return StartingAttempt(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=state.attempts_log + [attempt_result],
            )

    return JudgingAttempt(
        plan=state.plan,
        steps_log=state.steps_log,
        attempts_log=state.attempts_log,
        attempt_summary=state.attempt_summary,
    )


@log_call(include_args=["step_summary"])
async def _evaluate_step(
    settings: Settings, step_summary: Optional[str]
) -> tuple[Optional[StepVerdict], Optional[str]]:
    eval_prompt = (
        f"Evaluate if these changes make progress on the task {repr(settings.task)}.\n"
        "Here is the summary of the changes, provided by their author:\n\n"
        f"{step_summary}\n\n"
        "Here are the uncommitted changes:\n\n"
        f"{format_tool_code_output(await settings.env.run(['git', 'diff', '--', f':!{PLAN_FILE}'], directory=settings.cwd, run_timeout_seconds=settings.config.run_timeout_seconds), 'diff')}\n\n"
        "Here is the diff of the changes made in previous attempts:\n\n"
        f"{format_tool_code_output(await settings.env.run(['git', 'diff', settings.base_commit + '..HEAD', '--', f':!{PLAN_FILE}'], directory=settings.cwd, run_timeout_seconds=settings.config.run_timeout_seconds), 'diff')}\n\n"
        "After you are done, output your review as a single message using this template:\n\n"
        "    I am the step judge.\n\n"
        "    Feedback: [[your feedback on the work done]]\n\n"
        "    List of objections related to the already done work: [[list of objections concerning what was already done, or 'None']]\n\n"
        "    My feedback on your proposed next step: [[your feedback on the proposed next step, or 'None']]\n\n"
        "    Next step you should take: [[either the step proposed by the implementor, or the next unimplemented step from the plan, or 'Address the objections', or 'None']]\n\n"
        "    Verdict: [[your verdict]], end of step review.\n\n"
        "Your verdict must be one of the following:\n"
        "- SUCCESS SUCCESS SUCCESS if the changes are a good step forward and can be committed;\n"
        "- PARTIAL PARTIAL PARTIAL if the changes are somewhat helpful, but need more work;\n"
        "- FAILURE FAILURE FAILURE if the changes are not useful and the author must rethink the approach.\n"
    )

    if settings.config.implement.judge_extra_prompt:
        eval_prompt += f"\n\n{settings.config.implement.judge_extra_prompt}"

    settings.env.update_status("Evaluating step")
    evaluation = await settings.llm.run(
        settings.env, eval_prompt, yolo=True, cwd=settings.cwd, response_type=LLMOutputType.EVALUATION
    )
    verdict = check_verdict(StepVerdict, evaluation or "")
    return verdict, evaluation


@log_call(include_args=[])
async def _generate_commit_message(settings: Settings) -> str:
    """Generate and return a concise, single‑line commit message for the current step."""
    settings.env.update_status("Generating commit message")
    commit_msg_prompt = (
        f"Generate a concise commit message (max 15 words) for this step: {repr(settings.task)}.\n"
        "You *may not* output Markdown, code blocks, or any other formatting.\n"
        "You may only output a single line.\n"
    )
    commit_msg = await settings.llm.run(
        settings.env, commit_msg_prompt, yolo=False, cwd=settings.cwd, response_type=LLMOutputType.LLM_RESPONSE
    )
    if not commit_msg:
        commit_msg = "Step for task"
    return commit_msg


@log_call(include_args=["commit_msg"])
async def _commit_step(settings: Settings, commit_msg: str) -> None:
    """Stage and commit the changes for this step."""
    settings.env.update_status("Committing step")
    if await has_uncommitted_changes(settings.env, cwd=settings.cwd):
        await settings.env.run(
            ["git", "add", "."],
            "Adding files",
            directory=settings.cwd,
            run_timeout_seconds=settings.config.run_timeout_seconds,
        )
        await settings.env.run(
            ["git", "commit", "-m", f"{commit_msg[:100]}"],
            "Committing step",
            directory=settings.cwd,
            run_timeout_seconds=settings.config.run_timeout_seconds,
        )
    else:
        settings.env.log("No changes to commit.", message_type=LLMOutputType.STATUS)


@log_call(include_args=[])
async def _evaluate_task_completion(settings: Settings) -> tuple[Optional[TaskVerdict], Optional[str]]:
    """Ask the LLM whether the overall task is finished after this step."""
    settings.env.update_status("Checking if task is complete...")
    completion_prompt = (
        f"Is the task {repr(settings.task)} now complete based on the work done?\n"
        "You are granted access to tools, commands, and code execution for the *sole purpose* of evaluating whether the task is done.\n"
        "Here are the uncommitted changes:\n\n"
        f"{
            format_tool_code_output(
                await settings.env.run(
                    ['git', 'diff', '--', f':!{PLAN_FILE}'],
                    directory=settings.cwd,
                    run_timeout_seconds=settings.config.run_timeout_seconds,
                ),
                'diff',
            )
        }\n\n"
        "Here is the diff of the changes made in previous attempts:\n\n"
        f"{
            format_tool_code_output(
                await settings.env.run(
                    ['git', 'diff', settings.base_commit + '..HEAD', '--', f':!{PLAN_FILE}'],
                    directory=settings.cwd,
                    run_timeout_seconds=settings.config.run_timeout_seconds,
                ),
                'diff',
            )
        }\n\n"
        "After you are done, output your review as a single message using this template:\n\n"
        "    I am the task completion judge.\n\n"
        "    Task requirements: [[list of task requirements and for each - whether it was addressed]]\n\n"
        "    List of objections to address: [[list of objections concerning the implementation, or 'None']]\n\n"
        "    Verdict: [[your verdict]], end of task completion review.\n\n"
        "Your verdict must be one of the following:\n"
        "- COMPLETE COMPLETE COMPLETE if the task is fully done.\n"
        "- CONTINUE CONTINUE CONTINUE if more work is needed.\n"
    )

    if settings.config.implement.completion.judge_extra_prompt:
        completion_prompt += f"\n\n{settings.config.implement.completion.judge_extra_prompt}"

    completion_evaluation = await settings.llm.run(
        settings.env, completion_prompt, yolo=True, cwd=settings.cwd, response_type=LLMOutputType.EVALUATION
    )
    completion_verdict = check_verdict(TaskVerdict, completion_evaluation or "")
    return completion_verdict, completion_evaluation


async def _handle_JudgingStep(
    settings: Settings, state: JudgingStep
) -> StartingStep | FinalizingTask | StartingAttempt | RefiningPlan:
    # 1. generate commit message and commit the step
    commit_msg = await _generate_commit_message(settings)
    await _commit_step(settings, commit_msg)

    # 2. ask the LLM whether the task is done
    completion_verdict, completion_evaluation = await _evaluate_task_completion(settings)

    # 3. interpret the verdict and produce a StepPhaseResult
    if not completion_evaluation:
        settings.env.update_status("Failed to get a task completion evaluation.", style="red")
        settings.env.log("LLM provided no output", message_type=LLMOutputType.ERROR)
        return StartingStep(
            plan=state.plan,
            steps_log=state.steps_log
            + [
                StepResult(
                    verdict="failed",
                    feedback="Failed to get a task completion evaluation",
                    history=state.attempts_log,
                )
            ],
        )

    # TODO: there should be some retry thing specifically for getting verdicts, instead of just starting another step
    elif not completion_verdict:
        settings.env.update_status("Failed to get a task completion verdict.", style="red")
        settings.env.log(
            f"Couldn't determine the verdict from the task completion evaluation. Evaluation was:\n\n{completion_evaluation}",
            message_type=LLMOutputType.ERROR,
        )
        return StartingStep(
            plan=state.plan,
            steps_log=state.steps_log
            + [
                StepResult(
                    verdict="failed",
                    feedback="Couldn't determine the verdict from the task completion evaluation.",
                    history=state.attempts_log,
                )
            ],
        )

    match completion_verdict:
        case TaskVerdict.COMPLETE:
            settings.env.update_status("Task marked as complete.")
            settings.env.log("Task marked as complete", message_type=LLMOutputType.STATUS)
            return FinalizingTask(
                plan=state.plan,
                steps_log=state.steps_log
                + [
                    StepResult(
                        verdict=completion_verdict,
                        feedback=completion_evaluation,
                        history=state.attempts_log,
                    )
                ],
                verdict=completion_verdict,
                status=completion_evaluation,
            )

        case TaskVerdict.CONTINUE:
            settings.env.update_status("Task not complete, continuing step.")
            settings.env.log("Task not complete, continuing step", message_type=LLMOutputType.STATUS)
            # If there is feedback, go to RefiningPlan
            feedback = (
                completion_evaluation.strip() if completion_evaluation and completion_evaluation.strip() else None
            )
            if feedback:
                return RefiningPlan(
                    plan=state.plan,
                    steps_log=state.steps_log
                    + [
                        StepResult(
                            verdict=completion_verdict,
                            feedback=completion_evaluation,
                            history=state.attempts_log,
                        )
                    ],
                    feedback=feedback,
                )
            else:
                return StartingStep(
                    plan=state.plan,
                    steps_log=state.steps_log
                    + [
                        StepResult(
                            verdict=completion_verdict,
                            feedback=completion_evaluation,
                            history=state.attempts_log,
                        )
                    ],
                )

        case _:
            assert_never(completion_verdict)


async def _handle_RefiningPlan(settings: Settings, state: RefiningPlan) -> StartingStep:
    """
    Refine the plan using the previous plan and feedback, then continue with the new plan.
    """
    from ok.task_planning import planning_phase

    # Use the feedback and previous plan to get a revised plan
    revised_plan = await planning_phase(
        settings.env,
        task=settings.task,
        cwd=settings.cwd,
        llm=settings.llm,
        previous_plan=state.plan,
        previous_review=state.feedback,
    )
    # If planning fails, keep the old plan
    if not revised_plan:
        revised_plan = state.plan
    return StartingStep(
        plan=revised_plan,
        steps_log=state.steps_log,
    )


@log_call(include_args=["task", "base_commit", "cwd"])
async def implementation_phase(
    env: Env,
    *,
    task: str,
    base_commit: str,
    cwd: Path,
    llm: LLMBase,
) -> Done:
    """
    high‑level driver that repeatedly feeds events into the state‑machine
    until a terminal state is reached
    """
    env.set_phase("Step")
    env.log(
        f"Starting step phase for task: {task}",
        message_type=LLMOutputType.STATUS,
    )

    state: State = StartingTask()
    settings = Settings(
        env=env,
        task=task,
        base_commit=base_commit,
        cwd=cwd,
        llm=llm,
        config=env.config,
    )

    try:
        # kick‑off
        state = await transition(state, Tick(), settings)

        # main loop: keep working while we're in `Attempt`, `Evaluate`, or `ReviewCompletion`
        while not isinstance(state, Done):
            state = await transition(state, Tick(), settings)

    except* KeyboardInterrupt as group:
        env.log_debug("Caught an exception group", exc=[repr(e) for e in group.exceptions])
        settings.env.log(
            "Interrupted by user (KeyboardInterrupt)",
            message_type=LLMOutputType.ERROR,
        )
        settings.env.update_status("Interrupted by user.", style="red")
        # TODO: still do the final commit and everything

        match state:
            case FinalizingTask() | Done():
                status = state.status or ""
            case (
                StartingTask()
                | StartingStep()
                | StartingAttempt()
                | PostAttemptHooks()
                | JudgingAttempt()
                | JudgingStep()
                | RefiningPlan()
            ):
                status = ""
            case _:
                assert_never(state)

        state = Done(
            verdict="interrupted",
            status=status + "; Interrupted by user",
        )

    return state


# ────────────────────────────── Transitions ──────────────────────────────


async def _handle_JudgingAttempt(
    settings: Settings,
    state: JudgingAttempt,
) -> State:
    """
    This is called after each attempt to judge if the step is done
    """

    verdict, evaluation = await _evaluate_step(settings, state.attempt_summary)
    settings.env.log_debug("Verdict from judgment", verdict=verdict)

    if not verdict:
        attempt_result = AttemptResult(
            verdict=None,
            feedback=evaluation or "Failed to evaluate step",
        )
        new_attempts_log = state.attempts_log + [attempt_result]
        if no_progress_in_last_n(new_attempts_log, n=5):
            settings.env.log("No progress in 5 attempts, terminating.", message_type=LLMOutputType.ERROR)
            return Done(verdict="failed", status="No progress in 5 attempts")
        return StartingAttempt(
            plan=state.plan,
            steps_log=state.steps_log,
            attempts_log=new_attempts_log,
        )

    # 3️⃣  branch on verdict ------------------------------------------------------
    attempt_result = AttemptResult(
        verdict=verdict,
        feedback=evaluation,
    )
    new_attempts_log = state.attempts_log + [attempt_result]
    if no_progress_in_last_n(new_attempts_log, n=5):
        settings.env.log("No progress in 5 attempts, terminating.", message_type=LLMOutputType.ERROR)
        return Done(verdict="failed", status="No progress in 5 attempts")

    match verdict:
        case StepVerdict.SUCCESS:
            return JudgingStep(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=new_attempts_log,
            )
        case StepVerdict.PARTIAL | StepVerdict.AUTO_CHECK_FAILED:
            return StartingAttempt(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=new_attempts_log,
            )
        case StepVerdict.FAILURE:
            return StartingAttempt(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=new_attempts_log,
            )
        case other:
            assert_never(other)


async def _handle_FinalizingTask(settings: Settings, state: FinalizingTask) -> Done:
    try:
        diff = await settings.env.run(
            ["git", "diff", "--quiet"],
            "Checking for uncommitted changes",
            directory=settings.cwd,
            run_timeout_seconds=settings.config.run_timeout_seconds,
        )
        if not diff.success:
            await settings.env.run(
                ["git", "add", "."],
                "Staging uncommitted changes",
                directory=settings.cwd,
                run_timeout_seconds=settings.config.run_timeout_seconds,
            )
            await settings.env.run(
                ["git", "commit", "-m", "Final commit (auto)"],
                "Final commit after step phase",
                directory=settings.cwd,
                run_timeout_seconds=settings.config.run_timeout_seconds,
            )
    except Exception as e:
        settings.env.log_debug("Caught an exception", exc=repr(e))
        settings.env.log(f"Failed to make final commit: {e}", message_type=LLMOutputType.TOOL_ERROR)

    return Done(
        verdict=state.verdict,
        status=state.status,
    )


def no_progress_in_last_n(attempts_log, n=5):
    """
    Return True if there is no progress in the last n attempts.
    Progress is only SUCCESS, or a flip between PARTIAL and AUTO_CHECK_FAILED.
    If verdicts are all the same and not SUCCESS, or just flip endlessly, return True.
    """
    if len(attempts_log) < n:
        return False
    last_verdicts = [a.verdict for a in attempts_log[-n:]]
    # If any is SUCCESS, progress is made
    if any(v == StepVerdict.SUCCESS for v in last_verdicts):
        return False
    # If verdicts just flip between PARTIAL and AUTO_CHECK_FAILED, and nothing else
    allowed = {StepVerdict.PARTIAL, StepVerdict.AUTO_CHECK_FAILED}
    if set(last_verdicts).issubset(allowed):
        # Check if they just alternate
        pattern1 = [StepVerdict.PARTIAL, StepVerdict.AUTO_CHECK_FAILED] * (n // 2 + 1)
        pattern2 = [StepVerdict.AUTO_CHECK_FAILED, StepVerdict.PARTIAL] * (n // 2 + 1)
        if last_verdicts == pattern1[:n] or last_verdicts == pattern2[:n]:
            return True
    return False
