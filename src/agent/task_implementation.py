"""Manages the iterative implementation phase of a task, including code generation, execution, and evaluation."""

from enum import Enum
from pathlib import Path
from typing import Optional, TypedDict, assert_never

from .config import AGENT_SETTINGS as config
from .constants import PLAN_FILE
from .llm import LLM, check_verdict
from .output_formatter import LLMOutputType, print_formatted_message
from .ui import status_manager
from .utils import format_tool_code_output, log, run


class ImplementationResult(TypedDict):
    status: str
    feedback: str


def implementation_phase(
    *,
    task: str,
    plan: str,
    base_commit: str,
    cwd: Path,
    llm: LLM,
) -> ImplementationResult:
    """
    Manages the iterative implementation phase of a task.

    This function guides the agent through generating code, executing commands,
    and evaluating the results, with a mechanism for early bailout if progress
    stalls.

    The core loop involves:
    1.  **Generating Implementation:** The LLM is prompted with the task, plan, and
        previous feedback to generate the next implementation step. This prompt
        (`impl_prompt`) includes details about the current attempt number and
        expects a concise summary of the changes made.
    2.  **Post-Implementation Hook (Optional):** If configured, a shell command
        (`config.post_implementation_hook_command`) is executed after the LLM
        provides an implementation summary. This can be used for automated
        checks or actions.
    3.  **Evaluating Implementation:** The LLM is then prompted to evaluate the
        generated implementation. This evaluation prompt (`eval_prompt`) includes
        the implementation summary, uncommitted changes (`git diff`), and changes
        from previous commits (`git diff base_commit..HEAD`). The LLM is expected
        to return a verdict (SUCCESS, PARTIAL, FAILURE) and corresponding feedback.
    4.  **Handling Verdicts:**
        *   **SUCCESS:** If the implementation is successful, a commit message is
            generated by the LLM, and the changes are committed. Then, the LLM
            is prompted to check if the overall task is complete (`completion_prompt`).
            If complete, the phase ends; otherwise, the loop continues with the
            new feedback.
        *   **PARTIAL:** If the implementation makes partial progress, the loop
            continues with the feedback provided by the LLM.
        *   **FAILURE:** If the implementation fails, a consecutive failure counter
            is incremented. If this counter reaches a threshold (`max_consecutive_failures`),
            the implementation phase bails out. Otherwise, the loop continues with
            the feedback.
    5.  **Bailout Conditions:** The loop can bail out early due to:
        *   Reaching `max_implementation_attempts`.
        *   Reaching `max_consecutive_failures`.
        *   No commits being made after a certain number of attempts (`attempts >= 5 and commits_made == 0`).
        *   User interruption (KeyboardInterrupt).
        *   Unhandled exceptions.

    Finally, a cleanup step attempts to commit any remaining uncommitted changes.

    Args:
        task: The description of the task being implemented.
        plan: The implementation plan generated in the planning phase.
        base_commit: The Git commit SHA to base the implementation branch on.
        cwd: The current working directory for task execution as a Path.

    Returns:
        A dictionary containing the status of the implementation and any feedback.
    """
    status_manager.set_phase("Implementation")
    print_formatted_message(f"Starting implementation phase for task: {task}", message_type=LLMOutputType.STATUS)

    max_implementation_attempts = 10
    max_consecutive_failures = 3
    consecutive_failures = 0
    commits_made = 0

    feedback: Optional[str] = None
    result: Optional[ImplementationResult] = None
    try:
        for attempt in range(1, max_implementation_attempts + 1):
            status_manager.set_phase("Implementation", f"{attempt}/{max_implementation_attempts}")
            print_formatted_message(f"Implementation attempt {attempt}", message_type=LLMOutputType.STATUS)

            # Ask Gemini to implement next step
            impl_prompt = (
                f"Execution phase. You are implementing this task: {repr(task)}. This is your attempt #{attempt} out of {max_implementation_attempts}.\n\n"
                "Based on this plan:\n\n"
                f"{plan}\n\n"
                f"{f'And the feedback about your previous attempt:\n\n{feedback}\n\n' if feedback else ''}"
                f"Implement the next step for task {repr(task)}.\n"
                "Create files, run commands, and/or write code as needed.\n"
                "When done, output a concise summary of what you did, starting with 'My summary of the implementation:'.\n"
                "Your response will help the reviewer of your implementation understand the changes made.\n"
                "Finish your response with 'This is the end of the implementation summary'.\n"
            )

            if config.implement.extra_prompt:
                impl_prompt += f"\n\n{config.implement.extra_prompt}"

            status_manager.update_status("Getting implementation")
            implementation_summary = llm.run(impl_prompt, yolo=True, cwd=cwd, response_type=LLMOutputType.LLM_RESPONSE)

            if not implementation_summary:
                status_manager.update_status("Failed to get implementation.", style="red")
                log("Failed to get implementation", message_type=LLMOutputType.ERROR)
                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    log("Too many consecutive failures, giving up", message_type=LLMOutputType.ERROR)
                    result = {
                        "status": "failed",
                        "feedback": "Too many consecutive failures to get implementation from Gemini",
                    }
                    break
                continue

            if config.post_implementation_hook_command:
                run(
                    config.post_implementation_hook_command,
                    "Running post-implementation hook command",
                    directory=cwd,
                    shell=True,
                )

            # SUCCESS, PARTIAL, FAILURE correspond to the 'ImplementationVerdict' enum
            eval_prompt = (
                f"Evaluate if this implementation makes progress on the task {repr(task)}.\n"
                "The first line of your response must be:\n"
                "  - SUCCESS SUCCESS SUCCESS if it's a good step forward;\n"
                "  - PARTIAL PARTIAL PARTIAL if it's somewhat helpful;\n"
                "  - FAILURE FAILURE FAILURE if it's not useful.\n"
                "For the 'success' verdict, provide a brief comment on the implementation.\n"
                "For the 'partial' verdict, provide specific feedback on what could be improved or what remains to be done.\n"
                "For the 'failure' verdict, list specific reasons why the implementation is inadequate.\n"
                "The implementation is either in the uncommitted changes, in the previous commits, or both.\n"
                "Here is the summary of the implementation:\n\n"
                f"{implementation_summary}\n\n"
                "Here are the uncommitted changes:\n\n"
                f"{format_tool_code_output(run(['git', 'diff', '--', f':!{PLAN_FILE}'], directory=cwd), 'diff')}\n\n"
                "Here is the diff of the changes made in previous commits:\n\n"
                f"{format_tool_code_output(run(['git', 'diff', base_commit + '..HEAD', '--', f':!{PLAN_FILE}'], directory=cwd), 'diff')}"
            )

            if config.implement.judge_extra_prompt:
                eval_prompt += f"\n\n{config.implement.judge_extra_prompt}"

            status_manager.update_status("Evaluating implementation")
            evaluation = llm.run(eval_prompt, yolo=True, cwd=cwd, response_type=LLMOutputType.EVALUATION)
            verdict = check_verdict(ImplementationVerdict, evaluation or "")

            if not evaluation or verdict is None:
                status_manager.update_status("Failed to get a verdict.", style="red")
                if evaluation:
                    log(
                        f"Couldn't determine the verdict from the evaluation. Evaluation was:\n\n{evaluation}",
                        message_type=LLMOutputType.ERROR,
                    )
                else:
                    log("LLM provided no output", message_type=LLMOutputType.ERROR)
                consecutive_failures += 1
                # If we have too many consecutive failures, give up and exit the loop
                if consecutive_failures >= max_consecutive_failures:
                    log("Too many consecutive failures, giving up", message_type=LLMOutputType.ERROR)
                    result = {
                        "status": "failed",
                        "feedback": "Too many consecutive failures to get evaluation from Gemini",
                    }
                    break
                else:
                    # Ok let's try again, go back to the start of the loop
                    continue

            # We have our verdict
            feedback = evaluation  # Store feedback for next iteration

            if verdict == ImplementationVerdict.SUCCESS:
                status_manager.update_status(f"Successful (attempt {attempt}).")
                log(f"Implementation successful in attempt {attempt}", message_type=LLMOutputType.STATUS)
                consecutive_failures = 0
                commits_made += 1

                # Generate commit message and commit
                status_manager.update_status("Generating commit message")
                commit_msg_prompt = (
                    f"Generate a concise commit message (max 15 words) for this implementation step: {repr(task)}.\n"
                    "You *may not* output Markdown, code blocks, or any other formatting.\n"
                    "You may only output a single line.\n"
                )
                commit_msg = llm.run(commit_msg_prompt, yolo=False, cwd=cwd, response_type=LLMOutputType.LLM_RESPONSE)
                if not commit_msg:
                    commit_msg = "Implementation step for task"

                status_manager.update_status("Committing implementation")
                run(["git", "add", "."], "Adding implementation files", directory=cwd)
                run(
                    ["git", "commit", "-m", f"{commit_msg[:100]}"],
                    "Committing implementation",
                    directory=cwd,
                )

                # Check if task is complete
                status_manager.update_status("Checking if task is complete...")
                completion_prompt = (
                    f"Is the task {repr(task)} now complete based on the work done?\n"
                    "You are granted access to tools, commands, and code execution for the *sole purpose* of evaluating whether the task is done.\n"
                    "You may not finish your response at 'I have to check ...' or 'I have to inspect files ...' - you must use your tools to check directly.\n"
                    "The first line of your response must be:\n"
                    "  - COMPLETE COMPLETE COMPLETE if the task is fully done;\n"
                    "  - CONTINUE CONTINUE CONTINUE if more work is needed.\n"
                    "If 'continue', provide specific next steps to take, or objections to address.\n"
                    "Here are the uncommitted changes:\n\n"
                    f"{format_tool_code_output(run(['git', 'diff', '--', f':!{PLAN_FILE}'], directory=cwd), 'diff')}\n\n"
                    "Here is the diff of the changes made in previous commits:\n\n"
                    f"{format_tool_code_output(run(['git', 'diff', base_commit + '..HEAD', '--', f':!{PLAN_FILE}'], directory=cwd), 'diff')}"
                )

                if config.implement.completion.judge_extra_prompt:
                    completion_prompt += f"\n\n{config.implement.completion.judge_extra_prompt}"

                completion_evaluation = llm.run(
                    completion_prompt, yolo=True, cwd=cwd, response_type=LLMOutputType.EVALUATION
                )
                completion_verdict = check_verdict(TaskCompletionVerdict, completion_evaluation or "")

                if not completion_evaluation:
                    status_manager.update_status("Failed to get a task completion evaluation.", style="red")
                    log("LLM provided no output", message_type=LLMOutputType.ERROR)

                elif not completion_verdict:
                    status_manager.update_status("Failed to get a task completion verdict.", style="red")
                    log(
                        f"Couldn't determine the verdict from the task completion evaluation. Evaluation was:\n\n{completion_evaluation}",
                        message_type=LLMOutputType.ERROR,
                    )

                elif completion_verdict == TaskCompletionVerdict.COMPLETE:
                    status_manager.update_status("Task marked as complete.")
                    log("Task marked as complete", message_type=LLMOutputType.STATUS)
                    result = {"status": "completed", "feedback": completion_evaluation}
                    break

                elif completion_verdict == TaskCompletionVerdict.CONTINUE:
                    status_manager.update_status("Task not complete, continuing implementation.")
                    log("Task not complete, continuing implementation", message_type=LLMOutputType.STATUS)
                    feedback = completion_evaluation

                else:
                    assert_never(completion_verdict)

            elif verdict == ImplementationVerdict.PARTIAL:
                status_manager.update_status(f"Partial progress (attempt {attempt}).")
                log(f"Partial progress in attempt {attempt}", message_type=LLMOutputType.STATUS)

            elif verdict == ImplementationVerdict.FAILURE:
                status_manager.update_status(f"Failed (attempt {attempt}).", style="red")
                log(f"Implementation failed in attempt {attempt}", message_type=LLMOutputType.ERROR)
                consecutive_failures += 1
                if consecutive_failures >= max_consecutive_failures:
                    log("Too many consecutive failures, giving up", message_type=LLMOutputType.ERROR)
                    result = {"status": "failed", "feedback": "Too many consecutive failures in implementation"}
                    break

            else:
                # Typechecker will warn us if we miss a case
                assert_never(verdict)

            # Check if we've made no commits recently
            if attempt >= 5 and commits_made == 0:
                log("No commits made in 5 attempts, giving up", message_type=LLMOutputType.ERROR)
                result = {"status": "failed", "feedback": "No commits made in 5 attempts"}
                break
        else:
            # If we exit the loop without breaking, it means we reached max attempts
            log(
                f"Implementation incomplete after {max_implementation_attempts} attempts",
                message_type=LLMOutputType.ERROR,
            )
            status_manager.update_status("Incomplete.", style="red")
            return {"status": "incomplete", "feedback": "Implementation incomplete after maximum attempts"}

    except KeyboardInterrupt:
        log("Implementation interrupted by user (KeyboardInterrupt)", message_type=LLMOutputType.ERROR)
        status_manager.update_status("Interrupted by user.", style="red")
        result = {"status": "interrupted", "feedback": "Implementation interrupted by user"}
    except Exception as e:
        log(f"Implementation failed: {e}", message_type=LLMOutputType.ERROR)
        status_manager.update_status("Implementation failed.", style="red")
        result = {"status": "failed", "feedback": str(e)}
    finally:
        try:
            diff = run(["git", "diff", "--quiet"], "Checking for uncommitted changes", directory=cwd)
            if not diff["success"]:
                run(["git", "add", "."], "Adding remaining files before final commit", directory=cwd)
                run(
                    ["git", "commit", "-m", "Final commit (auto)"],
                    "Final commit after implementation phase",
                    directory=cwd,
                )
        except Exception as e:
            log(f"Failed to make final commit: {e}", message_type=LLMOutputType.TOOL_ERROR)
    return result


class ImplementationVerdict(Enum):
    """Enum for possible verdicts from the implementation judge."""

    SUCCESS = "SUCCESS"
    PARTIAL = "PARTIAL"
    FAILURE = "FAILURE"


class TaskCompletionVerdict(Enum):
    """Enum for possible verdicts from the task completion judge."""

    COMPLETE = "COMPLETE"
    """Task is fully completed."""
    CONTINUE = "CONTINUE"
    """More work is needed."""
