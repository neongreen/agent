"""
Manages the iterative step phase of a task, including code generation, execution, and evaluation.

Overview of the state machine:

- we begin a "step"
- we make an "attempt" to implement the step
- we judge the attempt
    - if the attempt is successful, we will be judging the step next
    - if the attempt is partial, we will be making another attempt
    - if the attempt is a failure, we will be making another attempt unless some limits are reached
- once we *either* exhausted the attempts or the last attempt was successful, we judge whether the step is complete
    - if the step is complete, we commit the changes
    - if the step is not complete, we will be making another attempt
- once the step is complete, we will be judging whether the task is complete
    - if the task is complete, we will be returning to the main loop
    - if the task is not complete, we will be starting a new step
"""

from __future__ import annotations

from dataclasses import dataclass
from enum import Enum
from itertools import takewhile
from pathlib import Path
from typing import Literal, Optional, assert_never

from eliot import log_call

from agent.config import AGENT_SETTINGS as config
from agent.constants import PLAN_FILE
from agent.git_utils import has_uncommitted_changes
from agent.llm import LLM, check_verdict
from agent.logging import LLMOutputType, log
from agent.task_planning import planning_phase
from agent.ui import status_manager
from agent.utils import format_tool_code_output, run


class StepVerdict(Enum):
    """Enum for possible verdicts from the step judge."""

    SUCCESS = "SUCCESS"
    """Work done here is a good step forward."""
    PARTIAL = "PARTIAL"
    """Keep work done in this step so far, but it needs more iteration."""
    FAILURE = "FAILURE"
    """Work done in this step is not useful and should be discarded."""


class TaskVerdict(Enum):
    """Enum for possible verdicts from the task completion judge."""

    COMPLETE = "COMPLETE"
    """Task is fully completed."""
    CONTINUE = "CONTINUE"
    """More work is needed."""


@dataclass(frozen=True, slots=True)
class AttemptResult:
    """Result of a single attempt in a step."""

    verdict: Optional[StepVerdict]
    feedback: Optional[str]


@dataclass(frozen=True, slots=True)
class StepResult:
    """Result we get after completing a *step*."""

    history: list[AttemptResult]
    verdict: TaskVerdict | Literal["failed"]
    feedback: Optional[str] = None


# Base state classes


@dataclass(frozen=True, slots=True)
class TaskState:
    """Base class for all states."""

    plan: str
    """Plan for the task, generated by the LLM."""

    steps_log: list[StepResult]
    """Previous steps' results."""


@dataclass(frozen=True, slots=True)
class StepState(TaskState):
    """Base class for all states happening while working on a step."""

    attempts_log: list[AttemptResult]
    """Previous attempts' results."""


@dataclass(frozen=True, slots=True)
class AttemptState(StepState):
    pass


# State machine states


@dataclass(frozen=True, slots=True)
class StartingTask:
    """
    Represents the initial state where the agent is ready to start working on a task.
    """


@dataclass(frozen=True, slots=True)
class StartingStep(TaskState):
    """
    Represents the state where the agent is ready to start working on a step.
    """


@dataclass(frozen=True, slots=True)
class StartingAttempt(StepState):
    """Represents the state where the agent is "inside" a step and is ready to make an attempt."""


@dataclass(frozen=True, slots=True)
class PostAttemptHooks(AttemptState):
    """
    The agent is going to run post-attempt hooks like `post-implementation-hook-command`
    and `post-implementation-check-command`.
    """

    attempt_summary: Optional[str]


@dataclass(frozen=True, slots=True)
class JudgingAttempt(AttemptState):
    """
    Represents the state where the agent reviews the work done and whether more attempts are needed.

    From here we can go to:
    - `MakingAttempt` to make another attempt if the step is not done yet
    - `JudgingStep` to see if the task is done or we need more steps
    """

    attempt_summary: Optional[str]
    """
    Natural‑language summary of the work done in the current attempt.

    `None` if the attempt failed to produce a summary, which may happen even if code changes were made.
    """


@dataclass(frozen=True, slots=True)
class JudgingStep(StepState):
    """
    After a successful step, commit the changes and ask whether the overall task is finished.

    From here we can go to:
    - `StartingStep` to start a new step if the task is not done yet
    - `FinalizingTask` to finalize the task
    """


@dataclass(frozen=True, slots=True)
class FinalizingTask(TaskState):
    """
    Maybe we need to do a few more things before saying we are "done".
    """

    verdict: TaskVerdict | Literal["failed"]
    status: Optional[str]


@dataclass(frozen=True, slots=True)
class Done:
    """
    Ok, now we're done.
    """

    verdict: TaskVerdict | Literal["failed", "interrupted"]
    status: Optional[str]


type State = (
    StartingTask
    | StartingStep
    | StartingAttempt
    | PostAttemptHooks
    | JudgingAttempt
    | JudgingStep
    | FinalizingTask
    | Done
)

# Events


@dataclass(frozen=True, slots=True)
class Tick:
    """Keep going."""


type Event = Tick


@dataclass(frozen=True, slots=True)
class Settings:
    task: str
    base_commit: str
    cwd: Path
    llm: LLM
    max_step_attempts: int = 10
    max_consecutive_failures: int = 3


@log_call(include_args=["state", "event"])
def transition(
    state: State,
    event: Event,
    settings: Settings,
) -> State:
    """
    single‑step transition for the task‑execution state‑machine

    all long‑running side‑effects (llm calls, git commands, etc.) are executed
    inside the relevant branches, so the caller only needs to keep feeding
    events until a terminal state (`Complete` or `Failed`) is reached
    """
    log(
        f"Entering transition with state: {state.__class__.__name__}, event: {event.__class__.__name__}",
        message_type=LLMOutputType.DEBUG,
    )
    match state, event:
        case StartingTask(), Tick():
            # TODO: this can actually be a class method?
            return _handle_StartingTask(settings, state)

        case StartingStep(), Tick():
            return _handle_StartingStep(settings, state)

        case StartingAttempt(), Tick():
            return _handle_StartingAttempt(settings, state)

        case PostAttemptHooks(), Tick():
            return _handle_PostAttemptHooks(settings, state)

        case JudgingAttempt(), Tick():
            return _handle_JudgingAttempt(settings, state)

        case JudgingStep(), Tick():
            return _handle_JudgingStep(settings, state)

        case FinalizingTask(), Tick():
            return _handle_FinalizingTask(settings, state)

        case Done(), Tick():
            log("Done state reached, no further transitions.", message_type=LLMOutputType.DEBUG)
            return state

        case _, _:
            assert_never(state)
            assert_never(event)


@log_call(include_args=["state"])
def _handle_StartingTask(settings: Settings, state: StartingTask) -> StartingStep | Done:
    """
    Generate a plan for the task.
    """

    # TODO: move into the same state machine?

    plan = planning_phase(llm=settings.llm, task=settings.task, cwd=settings.cwd)
    if not plan:
        log("Failed to generate a plan for the step", message_type=LLMOutputType.ERROR)
        return Done(
            verdict="failed",
            status="Failed to generate a plan for the step",
        )
    else:
        log("Plan generated successfully", message_type=LLMOutputType.STATUS)
        return StartingStep(
            plan=plan,
            steps_log=[],
        )


@log_call(include_args=["state"])
def _handle_StartingStep(settings: Settings, state: StartingStep) -> StartingAttempt:
    """
    Start a new step with the given plan.
    """

    return StartingAttempt(
        plan=state.plan,
        steps_log=state.steps_log,
        attempts_log=[],
    )


@log_call(include_args=["state"])
def _handle_StartingAttempt(settings: Settings, state: StartingAttempt) -> PostAttemptHooks:
    """
    Generate the implementation prompt for a single step, invoke the LLM,
    and return its summary of work done.

    This may be rerun multiple times until it succeeds.
    Each run is called an "attempt".
    """

    from agent.logging import format_as_markdown_blockquote

    prev_attempt_feedback = (
        f"And the feedback about your previous attempt:\n\n{format_as_markdown_blockquote(state.attempts_log[-1].feedback or '')}\n\n"
        if state.attempts_log
        else ""
    )

    # TODO: get rid of settings, they can all be in TaskState
    impl_prompt = (
        f"Execution phase. You are implementing this task: {repr(settings.task)}.\n"
        f"This is your attempt #{len(state.attempts_log) + 1} out of {settings.max_step_attempts}.\n"
        "\n"
        "Based on this plan:\n"
        "\n"
        f"{state.plan}\n"
        "\n"
        f"{prev_attempt_feedback}"
        f"Implement the next step for task {repr(settings.task)}.\n"
        "Create files, run commands, and/or write code as needed.\n"
        "When done, output a concise summary of what you did, starting with 'My summary of the work done here:'.\n"
        "Your response will help the reviewer understand the changes made.\n"
        "Finish your response with 'This is the end of the summary'.\n"
    )

    if config.implement.extra_prompt:
        impl_prompt += f"""\n\n{config.implement.extra_prompt}"""

    status_manager.update_status("Implementing a step")

    attempt_summary = settings.llm.run(
        impl_prompt, yolo=True, cwd=settings.cwd, response_type=LLMOutputType.LLM_RESPONSE
    )

    return PostAttemptHooks(
        plan=state.plan,
        steps_log=state.steps_log,
        attempts_log=state.attempts_log,
        attempt_summary=attempt_summary,
    )


@log_call(include_args=["state"])
def _handle_PostAttemptHooks(settings: Settings, state: PostAttemptHooks) -> JudgingAttempt | StartingAttempt:
    # This one always runs and is supposed to have things like formatters, etc.
    if config.post_implementation_hook_command:
        run(
            config.post_implementation_hook_command,
            "Running post-step hook",
            directory=settings.cwd,
            shell=True,
        )

    # This one *checks* code after the attempt. If it fails, we will make another attempt.
    if config.post_implementation_check_command:
        check_result = run(
            config.post_implementation_check_command,
            "Running post-implementation check",
            directory=settings.cwd,
            shell=True,
        )
        if check_result.exit_code != 0:
            feedback = (
                f"Post-implementation check command failed with exit code {check_result.exit_code}.\n"
                + format_tool_code_output(check_result)
            )
            attempt_result = AttemptResult(
                verdict=StepVerdict.PARTIAL,
                feedback=feedback,
            )
            return StartingAttempt(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=state.attempts_log + [attempt_result],
            )

    return JudgingAttempt(
        plan=state.plan,
        steps_log=state.steps_log,
        attempts_log=state.attempts_log,
        attempt_summary=state.attempt_summary,
    )


@log_call(include_args=["step_summary"])
def _evaluate_step(settings: Settings, step_summary: Optional[str]) -> tuple[Optional[StepVerdict], Optional[str]]:
    eval_prompt = (
        f"Evaluate if these changes make progress on the task {repr(settings.task)}.\n"
        "Here is the summary of the changes, provided by their author:\n\n"
        f"{step_summary}\n\n"
        "Here are the uncommitted changes:\n\n"
        f"{format_tool_code_output(run(['git', 'diff', '--', f':!{PLAN_FILE}'], directory=settings.cwd), 'diff')}\n\n"
        "Here is the diff of the changes made in previous attempts:\n\n"
        f"{format_tool_code_output(run(['git', 'diff', settings.base_commit + '..HEAD', '--', f':!{PLAN_FILE}'], directory=settings.cwd), 'diff')}\n\n"
        "After you are done, output your review as a single message using this template:\n\n"
        "    I am the step judge.\n\n"
        "    Feedback: [[your feedback on the current batch of changes]]\n\n"
        "    List of objections related to the already done work: [[list of objections concerning what was already done, or 'None']]\n\n"
        "    Next step: [[either the next unimplemented step from the plan, or 'Address the objections', or 'None']]\n\n"
        "    Verdict: [[your verdict]], end of step review.\n\n"
        "Your verdict must be one of the following:\n"
        "- SUCCESS SUCCESS SUCCESS if the changes are a good step forward and can be committed;\n"
        "- PARTIAL PARTIAL PARTIAL if the changes are somewhat helpful, but need more work;\n"
        "- FAILURE FAILURE FAILURE if the changes are not useful and the author must rethink the approach.\n"
    )

    if config.implement.judge_extra_prompt:
        eval_prompt += f"\n\n{config.implement.judge_extra_prompt}"

    status_manager.update_status("Evaluating step")
    evaluation = settings.llm.run(eval_prompt, yolo=True, cwd=settings.cwd, response_type=LLMOutputType.EVALUATION)
    verdict = check_verdict(StepVerdict, evaluation or "")
    return verdict, evaluation


@log_call(include_args=[])
def _generate_commit_message(settings: Settings) -> str:
    """Generate and return a concise, single‑line commit message for the current step."""
    status_manager.update_status("Generating commit message")
    commit_msg_prompt = (
        f"Generate a concise commit message (max 15 words) for this step: {repr(settings.task)}.\n"
        "You *may not* output Markdown, code blocks, or any other formatting.\n"
        "You may only output a single line.\n"
    )
    commit_msg = settings.llm.run(
        commit_msg_prompt,
        yolo=False,
        cwd=settings.cwd,
        response_type=LLMOutputType.LLM_RESPONSE,
    )
    if not commit_msg:
        commit_msg = "Step for task"
    return commit_msg


@log_call(include_args=["commit_msg"])
def _commit_step(settings: Settings, commit_msg: str) -> None:
    """Stage and commit the changes for this step."""
    status_manager.update_status("Committing step")
    if has_uncommitted_changes(cwd=settings.cwd):
        run(["git", "add", "."], "Adding files", directory=settings.cwd)
        run(
            ["git", "commit", "-m", f"{commit_msg[:100]}"],
            "Committing step",
            directory=settings.cwd,
        )
    else:
        log("No changes to commit.", message_type=LLMOutputType.STATUS)


@log_call(include_args=[])
def _evaluate_task_completion(settings: Settings) -> tuple[Optional[TaskVerdict], Optional[str]]:
    """Ask the LLM whether the overall task is finished after this step."""
    status_manager.update_status("Checking if task is complete...")
    completion_prompt = (
        f"Is the task {repr(settings.task)} now complete based on the work done?\n"
        "You are granted access to tools, commands, and code execution for the *sole purpose* of evaluating whether the task is done.\n"
        "Here are the uncommitted changes:\n\n"
        f"{format_tool_code_output(run(['git', 'diff', '--', f':!{PLAN_FILE}'], directory=settings.cwd), 'diff')}\n\n"
        "Here is the diff of the changes made in previous attempts:\n\n"
        f"{format_tool_code_output(run(['git', 'diff', settings.base_commit + '..HEAD', '--', f':!{PLAN_FILE}'], directory=settings.cwd), 'diff')}\n\n"
        "After you are done, output your review as a single message using this template:\n\n"
        "    I am the task completion judge.\n\n"
        "    Task requirements: [[list of task requirements and for each - whether it was addressed]]\n\n"
        "    List of objections to address: [[list of objections concerning the implementation, or 'None']]\n\n"
        "    Verdict: [[your verdict]], end of task completion review.\n\n"
        "Your verdict must be one of the following:\n"
        "- COMPLETE COMPLETE COMPLETE if the task is fully done.\n"
        "- CONTINUE CONTINUE CONTINUE if more work is needed.\n"
    )

    if config.implement.completion.judge_extra_prompt:
        completion_prompt += f"\n\n{config.implement.completion.judge_extra_prompt}"

    completion_evaluation = settings.llm.run(
        completion_prompt,
        yolo=True,
        cwd=settings.cwd,
        response_type=LLMOutputType.EVALUATION,
    )
    completion_verdict = check_verdict(TaskVerdict, completion_evaluation or "")
    return completion_verdict, completion_evaluation


@log_call(include_args=["state"])
def _handle_JudgingStep(settings: Settings, state: JudgingStep) -> StartingStep | FinalizingTask | StartingAttempt:
    # 1. generate commit message and commit the step
    commit_msg = _generate_commit_message(settings)
    _commit_step(settings, commit_msg)

    # 2. ask the LLM whether the task is done
    completion_verdict, completion_evaluation = _evaluate_task_completion(settings)

    # 3. interpret the verdict and produce a StepPhaseResult
    if not completion_evaluation:
        status_manager.update_status("Failed to get a task completion evaluation.", style="red")
        log("LLM provided no output", message_type=LLMOutputType.ERROR)
        return StartingStep(
            plan=state.plan,
            steps_log=state.steps_log
            + [
                StepResult(
                    verdict="failed",
                    feedback="Failed to get a task completion evaluation",
                    history=state.attempts_log,
                )
            ],
        )

    # TODO: there should be some retry thing specifically for getting verdicts, instead of just starting another step
    elif not completion_verdict:
        status_manager.update_status("Failed to get a task completion verdict.", style="red")
        log(
            f"Couldn't determine the verdict from the task completion evaluation. Evaluation was:\n\n{completion_evaluation}",
            message_type=LLMOutputType.ERROR,
        )
        return StartingStep(
            plan=state.plan,
            steps_log=state.steps_log
            + [
                StepResult(
                    verdict="failed",
                    feedback="Couldn't determine the verdict from the task completion evaluation.",
                    history=state.attempts_log,
                )
            ],
        )

    match completion_verdict:
        case TaskVerdict.COMPLETE:
            status_manager.update_status("Task marked as complete.")
            log("Task marked as complete", message_type=LLMOutputType.STATUS)
            return FinalizingTask(
                plan=state.plan,
                steps_log=state.steps_log
                + [
                    StepResult(
                        verdict=completion_verdict,
                        feedback=completion_evaluation,
                        history=state.attempts_log,
                    )
                ],
                verdict=completion_verdict,
                status=completion_evaluation,
            )

        case TaskVerdict.CONTINUE:
            status_manager.update_status("Task not complete, continuing step.")
            log("Task not complete, continuing step", message_type=LLMOutputType.STATUS)
            return StartingStep(
                plan=state.plan,
                steps_log=state.steps_log
                + [
                    StepResult(
                        verdict=completion_verdict,
                        feedback=completion_evaluation,
                        history=state.attempts_log,
                    )
                ],
            )

        case _:
            assert_never(state)


@log_call(include_args=["task", "base_commit", "cwd"])
def implementation_phase(
    *,
    task: str,
    base_commit: str,
    cwd: Path,
    llm: LLM,
) -> Done:
    """
    high‑level driver that repeatedly feeds events into the state‑machine
    until a terminal state is reached
    """
    status_manager.set_phase("Step")
    log(
        f"Starting step phase for task: {task}",
        message_type=LLMOutputType.STATUS,
    )

    state: State = StartingTask()
    settings = Settings(
        task=task,
        base_commit=base_commit,
        cwd=cwd,
        llm=llm,
    )

    try:
        # kick‑off
        state = transition(state, Tick(), settings)

        # main loop: keep working while we're in `Attempt`, `Evaluate`, or `ReviewCompletion`
        while not isinstance(state, Done):
            state = transition(state, Tick(), settings)

    except KeyboardInterrupt:
        log(
            "Interrupted by user (KeyboardInterrupt)",
            message_type=LLMOutputType.ERROR,
        )
        status_manager.update_status("Interrupted by user.", style="red")
        # TODO: still do the final commit and everything

        match state:
            case FinalizingTask() | Done():
                status = state.status or ""
            case (
                StartingTask()
                | StartingStep()
                | StartingAttempt()
                | PostAttemptHooks()
                | JudgingAttempt()
                | JudgingStep()
            ):
                status = ""
            case _:
                assert_never(state)

        state = Done(
            verdict="interrupted",
            status=status + "; Interrupted by user",
        )

    return state


# ────────────────────────────── Transitions ──────────────────────────────


@log_call
def _is_failed_attempt(attempt: AttemptResult) -> bool:
    """
    Check if the attempt is a failed attempt.
    """
    match attempt.verdict:
        case StepVerdict.SUCCESS | StepVerdict.PARTIAL:
            return False
        case StepVerdict.FAILURE | None:
            return True
        case other:
            assert_never(other)


@log_call(include_args=["state"])
def _handle_JudgingAttempt(
    settings: Settings,
    state: JudgingAttempt,
) -> State:
    """
    This is called after each attempt to judge if the step is done
    """

    prev_failed_attempts = len(list(takewhile(_is_failed_attempt, reversed(state.attempts_log))))

    verdict, evaluation = _evaluate_step(settings, state.attempt_summary)
    log(f"Verdict from the judgment: {verdict}", message_type=LLMOutputType.DEBUG)

    if not verdict:
        attempt_result = AttemptResult(
            verdict=None,
            feedback=evaluation or "Failed to evaluate step",
        )
        if prev_failed_attempts >= settings.max_consecutive_failures:
            return JudgingStep(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=state.attempts_log + [attempt_result],
            )
        else:
            return StartingAttempt(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=state.attempts_log + [attempt_result],
            )

    # 3️⃣  branch on verdict ------------------------------------------------------
    attempt_result = AttemptResult(
        verdict=verdict,
        feedback=evaluation,
    )

    match verdict:
        case StepVerdict.SUCCESS:
            # hand over to the completion‑review state

            # 2. head over to the completion‑review state
            return JudgingStep(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=state.attempts_log + [attempt_result],
            )

        case StepVerdict.PARTIAL:
            return StartingAttempt(
                plan=state.plan,
                steps_log=state.steps_log,
                attempts_log=state.attempts_log + [attempt_result],
            )

        case StepVerdict.FAILURE:
            if prev_failed_attempts >= settings.max_consecutive_failures:
                log(
                    f"Reached maximum consecutive failures ({settings.max_consecutive_failures}), finishing this step.",
                    message_type=LLMOutputType.ERROR,
                )
                return JudgingStep(
                    plan=state.plan,
                    steps_log=state.steps_log,
                    attempts_log=state.attempts_log + [attempt_result],
                )
            else:
                return StartingAttempt(
                    plan=state.plan,
                    steps_log=state.steps_log,
                    attempts_log=state.attempts_log + [attempt_result],
                )

        case other:
            assert_never(other)


@log_call(include_args=["state"])
def _handle_FinalizingTask(settings: Settings, state: FinalizingTask) -> Done:
    try:
        diff = run(["git", "diff", "--quiet"], "Checking for uncommitted changes", directory=settings.cwd)
        if not diff.success:
            run(["git", "add", "."], "Staging uncommitted changes", directory=settings.cwd)
            run(
                ["git", "commit", "-m", "Final commit (auto)"],
                "Final commit after step phase",
                directory=settings.cwd,
            )
    except Exception as e:
        log(f"Failed to make final commit: {e}", message_type=LLMOutputType.TOOL_ERROR)

    return Done(
        verdict=state.verdict,
        status=state.status,
    )


# TODO: bring back checking for max step attempts
